{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Tokenlevel TermExtraction for ACL Dataset.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "261f3489b5fa40e5a418b2f7c86a7485": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f238cb2cebc8432db62e2ce58501d4ee",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_18298fcbc62342b1866ef1698a07479c",
              "IPY_MODEL_2ba1651850a540148c98e28b93fd9699"
            ]
          }
        },
        "f238cb2cebc8432db62e2ce58501d4ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "18298fcbc62342b1866ef1698a07479c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_db1dcda6745f45f3be4db5d7ca36b7e4",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 5069051,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 5069051,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_700cd22776464a0bade4f6ef18328d8d"
          }
        },
        "2ba1651850a540148c98e28b93fd9699": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_68a7382cd4d349fda83e520cd1bf409f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 5.07M/5.07M [00:06&lt;00:00, 748kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_97b9a7598222413e8c6715e2867ea1c8"
          }
        },
        "db1dcda6745f45f3be4db5d7ca36b7e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "700cd22776464a0bade4f6ef18328d8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "68a7382cd4d349fda83e520cd1bf409f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "97b9a7598222413e8c6715e2867ea1c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bce44ce6b1db444c847e91ef7ea2a65d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_bce2c67fe5bb4a61923dbfb27ce0f9be",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_bbfc007aa13b492d939693b75245be37",
              "IPY_MODEL_166e8522e25648bc824a7d8ebd05defe"
            ]
          }
        },
        "bce2c67fe5bb4a61923dbfb27ce0f9be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bbfc007aa13b492d939693b75245be37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ae1cbd96881f4debae04380388f732be",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 9096718,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 9096718,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8553e6e79ea34dce95d37af0a51232ac"
          }
        },
        "166e8522e25648bc824a7d8ebd05defe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ef5512996212415c8d23dcff350e9d20",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 9.10M/9.10M [00:04&lt;00:00, 2.20MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_57b1fe6b1cf642618cd942635a492f2f"
          }
        },
        "ae1cbd96881f4debae04380388f732be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8553e6e79ea34dce95d37af0a51232ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ef5512996212415c8d23dcff350e9d20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "57b1fe6b1cf642618cd942635a492f2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0736701407234906a42b83c2b44596d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_56a4fae342704759b094c7ce651d4b2a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7e633a8fd24b4c589ebe755ae113b097",
              "IPY_MODEL_12dd819648df4f12b1900ba2ed6755f8"
            ]
          }
        },
        "56a4fae342704759b094c7ce651d4b2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7e633a8fd24b4c589ebe755ae113b097": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a672ddfcc6ef4790817b9d1eabcdae3b",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 512,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 512,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7ba12e11c3f6459181dcdf13d0cea4c3"
          }
        },
        "12dd819648df4f12b1900ba2ed6755f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_011db152017047c4a29a6689c81e2142",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 512/512 [00:24&lt;00:00, 20.5B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e9358badb01642dfb08c116bb336ad15"
          }
        },
        "a672ddfcc6ef4790817b9d1eabcdae3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7ba12e11c3f6459181dcdf13d0cea4c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "011db152017047c4a29a6689c81e2142": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e9358badb01642dfb08c116bb336ad15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dbc12261a7f44edfa047ebd868fa3641": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8081057dab3e45a9853ac6e28cc32f43",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_10464b7d831f4ddf8c8ae25ea62144a0",
              "IPY_MODEL_8f7a30667b404648b893fb56dba5f81c"
            ]
          }
        },
        "8081057dab3e45a9853ac6e28cc32f43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "10464b7d831f4ddf8c8ae25ea62144a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_731fc42ab6aa44878e2fcfa6d9107fe4",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1115590446,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1115590446,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0ca1cbd92453441fba37b7717842b8cd"
          }
        },
        "8f7a30667b404648b893fb56dba5f81c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d53564f0c90247b3a5e6cbbcea36904d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.12G/1.12G [00:24&lt;00:00, 46.0MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b26ac46a11a344b3bf2fe2c4bb25f4f9"
          }
        },
        "731fc42ab6aa44878e2fcfa6d9107fe4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0ca1cbd92453441fba37b7717842b8cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d53564f0c90247b3a5e6cbbcea36904d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b26ac46a11a344b3bf2fe2c4bb25f4f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yMTmZptEkHC"
      },
      "source": [
        "# Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrBWcZPkIUJl"
      },
      "source": [
        "use_annotator_1=True  #if False then train/val/test on annotator 2"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCDic7wtFAvR",
        "outputId": "139735b4-4950-4208-9f4c-7960e76bf7b7"
      },
      "source": [
        "%cd drive/My\\ Drive/1\\ Job/Product\\ and\\ Code/TermExtraction\n",
        "!ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/1 Job/Product and Code/TermExtraction\n",
            " ACL\t\t\t\t    htfl_en.pickle\n",
            " ACTER-master\t\t\t    htfl_fr.pickle\n",
            " ACTER-master.zip\t\t    htfl_nl.pickle\n",
            " additionaltexts\t\t    logs\n",
            " additionaltexts_extracted_de.txt  'saved models'\n",
            " additionaltexts_extracted_EN.txt   train_data_corp_en_new.pkl\n",
            " extracted_terms_htfl\t\t    wnut17train.conll\n",
            " false_negatives.txt\t\t    wnut17train.conll.1\n",
            " false_positives.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaxWLY9GFE2W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fae7d241-c9e7-467c-cf67-98add95d203c"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install sacremoses\n",
        "!pip install sentencepiece\n",
        "!pip install seqeval"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/43/cfe4ee779bbd6a678ac6a97c5a5cdeb03c35f9eaebbb9720b036680f9a2d/transformers-4.6.1-py3-none-any.whl (2.2MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.3MB 9.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 901kB 30.7MB/s \n",
            "\u001b[?25hCollecting huggingface-hub==0.0.8\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.3MB 35.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (8.0.0)\n",
            "Installing collected packages: sacremoses, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.0.8 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.6.1\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (0.0.45)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses) (8.0.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacremoses) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sacremoses) (4.41.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.0.1)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.2MB 9.3MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.95\n",
            "Collecting seqeval\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/2d/233c79d5b4e5ab1dbf111242299153f3caddddbb691219f363ad55ce783d/seqeval-1.2.2.tar.gz (43kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51kB 4.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.0.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-cp37-none-any.whl size=16172 sha256=2c975fad2dd089d42f76600cc87afa81c81b3ca4f0fd73e45dea63b54088570b\n",
            "  Stored in directory: /root/.cache/pip/wheels/52/df/1b/45d75646c37428f7e626214704a0e35bd3cfc32eda37e59e5f\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9fYtB3_FHuK"
      },
      "source": [
        "#torch and tranformers for model and training\n",
        "import torch  \n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.utils.data import TensorDataset\n",
        "from transformers import XLMRobertaTokenizerFast              \n",
        "from transformers import XLMRobertaForTokenClassification\n",
        "from transformers import AdamW                            \n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from transformers import DataCollatorForTokenClassification\n",
        "from transformers import Trainer, TrainingArguments\n",
        "import sentencepiece\n",
        "\n",
        "#sklearn for evaluation\n",
        "from sklearn import preprocessing                       \n",
        "from sklearn.metrics import classification_report        \n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import ParameterGrid         \n",
        "from sklearn.model_selection import ParameterSampler      \n",
        "from sklearn.utils.fixes import loguniform\n",
        "\n",
        "#nlp preprocessing\n",
        "from nltk import ngrams                                 \n",
        "from spacy.pipeline import SentenceSegmenter\n",
        "from spacy.lang.en import English\n",
        "from spacy.pipeline import Sentencizer\n",
        "from sacremoses import MosesTokenizer, MosesDetokenizer\n",
        "\n",
        "\n",
        "#utilities\n",
        "from seqeval.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "import pandas as pd\n",
        "import glob, os\n",
        "import time\n",
        "import datetime\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "import seaborn as sns\n",
        "import pickle         # for saving data structures\n",
        "from pynvml import *  # for checking gpu memory"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bE5bg7CqxXR5",
        "outputId": "398a961a-5ef8-4862-82e2-1c5f1a8c1649"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " ACL\t\t\t\t    htfl_en.pickle\n",
            " ACTER-master\t\t\t    htfl_fr.pickle\n",
            " ACTER-master.zip\t\t    htfl_nl.pickle\n",
            " additionaltexts\t\t    logs\n",
            " additionaltexts_extracted_de.txt  'saved models'\n",
            " additionaltexts_extracted_EN.txt   train_data_corp_en_new.pkl\n",
            " extracted_terms_htfl\t\t    wnut17train.conll\n",
            " false_negatives.txt\t\t    wnut17train.conll.1\n",
            " false_positives.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AdNNsNoxYUR"
      },
      "source": [
        "!pip freeze > requirements_TermExtraction.txt"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyekDMZ28gBc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d86a7f6-3270-4d03-a7f3-cecc29ca829b"
      },
      "source": [
        "# connect to GPU \n",
        "device = torch.device('cuda')\n",
        "\n",
        "print('Connected to GPU:', torch.cuda.get_device_name(0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Connected to GPU: Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RPZ14sYHHUm"
      },
      "source": [
        "# Prepare Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sU7NMPaDvbWt"
      },
      "source": [
        "**Functions for preprocessing and creating of Training Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7iI3dAw7Bpl"
      },
      "source": [
        "#load sentence list \n",
        "with open(\"ACL/ACL2_annotator1_src.txt\") as file_in:\n",
        "    sentences_an1 = []\n",
        "    for line in file_in:\n",
        "        sentences_an1.append(line.strip())\n",
        "\n",
        "with open(\"ACL/ACL2_annotator2_src.txt\") as file_in:\n",
        "    sentences_an2 = []\n",
        "    for line in file_in:\n",
        "        sentences_an2.append(line.strip())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T664Jwa07D46"
      },
      "source": [
        "#load label list \n",
        "with open(\"ACL/ACL2_annotator1_label.txt\") as file_in:\n",
        "    labels_an1 = []\n",
        "    for line in file_in:\n",
        "        labels_an1.append(line.strip().split(\"\\t\"))\n",
        "\n",
        "with open(\"ACL/ACL2_annotator2_label.txt\") as file_in:\n",
        "    labels_an2 = []\n",
        "    for line in file_in:\n",
        "        labels_an2.append(line.strip().split(\"\\t\"))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nXtHwAyPoK0"
      },
      "source": [
        "# tokenize\n",
        "def preprocess(sentences):\n",
        "  sentences_tokenized=[]\n",
        "  mt = MosesTokenizer(lang='en')\n",
        "  for s in sentences:\n",
        "    tokenized_text = mt.tokenize(s, return_str=True)            #append tuple of tokens and original senteence\n",
        "    sentences_tokenized.append((tokenized_text.split(), s))     #append tuple of tokens and original senteence\n",
        "  return sentences_tokenized\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkX-cb_dCQf2"
      },
      "source": [
        "sentences_an1_tok = preprocess(sentences_an1)\n",
        "sentences_an2_tok = preprocess(sentences_an2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPUGg3GD9d5W"
      },
      "source": [
        "#find indices of a sublist sub in a list l\n",
        "def find_sub_list(subl,l):\n",
        "    results=[]\n",
        "    subllen=len(subl)\n",
        "    for ind in (i for i,e in enumerate(l) if e==subl[0]):\n",
        "        if l[ind:ind+subllen]==subl:\n",
        "            results.append((ind,ind+subllen-1))\n",
        "\n",
        "    return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wn_oTC0vGDmW"
      },
      "source": [
        "#input is list of sentences and a list of corresponding terms\n",
        "def create_training_data(sentences, terms):\n",
        "\n",
        "  #create empty dataframe\n",
        "  training_data = []\n",
        "  \n",
        "  md = MosesDetokenizer(lang='en')\n",
        "\n",
        "  print(len(sentences))\n",
        "  count=0\n",
        "\n",
        "  for i in range(len(sentences)):\n",
        "    count+=1\n",
        "    #if count%100==0:print(count)\n",
        "\n",
        "    s=sentences[i][0]  \n",
        "\n",
        "    #create label list, with \"n\" for non-terms, \"B-T\" for beginning of a term and \"T\" for the continuation of a term\n",
        "    tags=[\"n\"]*len(s)\n",
        "\n",
        "    # check all terms\n",
        "    for t in terms[i]:\n",
        "      #find indices of term in sentence token list\n",
        "      t_as_list=t.split()\n",
        "      #print(t_as_list, s)\n",
        "      if len(t_as_list)>0:\n",
        "        sublist_indices=find_sub_list(t_as_list, s)\n",
        "        for indices in sublist_indices:\n",
        "          for ind in range(indices[0],indices[1]+1):\n",
        "            #if term start\n",
        "            if ind==indices[0]:\n",
        "              tags[ind]=\"B-T\"\n",
        "            #if continuation of a Term\n",
        "            else: \n",
        "              tags[ind]=\"T\"\n",
        "\n",
        "    training_data.append((s,tags))\n",
        "        \n",
        "\n",
        "  return training_data\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5bOYAbsRZxw",
        "outputId": "4c0a9969-8635-4492-fd31-57cf8b2a82b2"
      },
      "source": [
        "data_an1=create_training_data(sentences_an1_tok, labels_an1)\n",
        "data_an2=create_training_data(sentences_an2_tok, labels_an2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "900\n",
            "1301\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMCthdJPdsUL",
        "outputId": "bd95b34d-9ab2-4bc2-967f-0b1d1dff4d13"
      },
      "source": [
        "len(labels_an1)==len(data_an1) and len(labels_an1)==len(sentences_an1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-DlSvkEg-Xr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c195f47-6f16-4a45-f980-e7ee7f01d83f"
      },
      "source": [
        "len(labels_an2)==len(data_an2) and len(labels_an2)==len(sentences_an2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSy8hZggPQpf",
        "outputId": "4ade48d4-269e-4552-d784-ec4f3af6bd5f"
      },
      "source": [
        "#train val test split either with data of annotator 1 or 2\n",
        "\n",
        "if use_annotator_1:\n",
        "  trainings_data = data_an1[:540]\n",
        "  val_data = data_an1[540:720]\n",
        "  test_data = data_an1[720:]\n",
        "\n",
        "  gold_set_for_validation=set()\n",
        "  gold_set_for_test=set()\n",
        "\n",
        "  for terms in labels_an1[540:720]:\n",
        "    for t in terms:\n",
        "      if t not in gold_set_for_validation:\n",
        "        gold_set_for_validation.add(t)\n",
        "\n",
        "  for terms in labels_an1[720:]:\n",
        "    for t in terms:\n",
        "      if t not in gold_set_for_test:\n",
        "        gold_set_for_test.add(t)\n",
        "\n",
        "else:\n",
        "  trainings_data = data_an2[:781]\n",
        "  val_data = data_an2[781:1041]\n",
        "  test_data = data_an2[1041:]\n",
        "\n",
        "  gold_set_for_validation=set()\n",
        "  gold_set_for_test=set()\n",
        "\n",
        "  for terms in labels_an2[781:1041]:\n",
        "    for t in terms:\n",
        "      if t not in gold_set_for_validation:\n",
        "        gold_set_for_validation.add(t)\n",
        "\n",
        "  for terms in labels_an2[1041:]:\n",
        "    for t in terms:\n",
        "      if t not in gold_set_for_test:\n",
        "        gold_set_for_test.add(t)\n",
        "\n",
        "print(\"An1\", use_annotator_1)\n",
        "print(\"Train\",len(trainings_data))\n",
        "print(\"Val\",len(val_data))\n",
        "print(\"Test\",len(test_data))\n",
        "print(\"Terms Val\",len(gold_set_for_validation))\n",
        "print(\"Terms Test\",len(gold_set_for_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "An1 True\n",
            "Train 540\n",
            "Val 180\n",
            "Test 180\n",
            "Terms Val 421\n",
            "Terms Test 478\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0QIDvwX6YKc",
        "outputId": "365e3b50-bd79-44a5-e916-03ba40acd117"
      },
      "source": [
        "#look for some example sentences that contain multi word term\n",
        "for i in range(100):\n",
        "  if len(test_data[i][0])<12 and \"T\" in test_data[i][1]:\n",
        "    print(test_data[i][0])\n",
        "    print(test_data[i][1])\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Secondly', ',', 'we', 'exhibit', 'a', 'provably', 'optimal', 'free', 'indexation', 'algorithm', '.']\n",
            "['n', 'n', 'n', 'n', 'n', 'n', 'n', 'B-T', 'T', 'T', 'n']\n",
            "\n",
            "['This', 'paper', 'introduces', 'a', 'robust', 'interactive', 'method', 'for', 'speech', 'understanding', '.']\n",
            "['n', 'n', 'n', 'n', 'n', 'B-T', 'T', 'T', 'T', 'T', 'n']\n",
            "\n",
            "['The', 'generalized', 'LR', 'parsing', 'is', 'enhanced', 'in', 'this', 'approach', '.']\n",
            "['n', 'B-T', 'T', 'T', 'n', 'n', 'n', 'n', 'n', 'n']\n",
            "\n",
            "['A', 'pilot', 'system', 'has', 'shown', 'great', 'effectiveness', 'of', 'this', 'approach', '.']\n",
            "['n', 'B-T', 'T', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n']\n",
            "\n",
            "['The', 'interpretation', 'reflects', 'the', 'temporary', 'belief', 'about', 'the', 'world', '.']\n",
            "['n', 'n', 'n', 'n', 'B-T', 'T', 'n', 'n', 'B-T', 'n']\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdoXY46fSoxS"
      },
      "source": [
        "#seperate tokens and tags\n",
        "\n",
        "#train\n",
        "train_tags=[tup[1] for tup in trainings_data]\n",
        "train_texts=[tup[0] for tup in trainings_data]\n",
        "\n",
        "#val\n",
        "val_tags=[tup[1] for tup in val_data]\n",
        "val_texts=[tup[0] for tup in val_data]\n",
        "\n",
        "#test\n",
        "test_tags=[tup[1] for tup in test_data]\n",
        "test_texts=[tup[0] for tup in test_data]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUwUKpQyZ_GD",
        "outputId": "ead7b7f5-b813-4244-8842-cc80066c5cd4"
      },
      "source": [
        "val_texts[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Their',\n",
              " 'human',\n",
              " 'listeners',\n",
              " 'are',\n",
              " 'usually',\n",
              " 'able',\n",
              " 'to',\n",
              " 'cope',\n",
              " 'with',\n",
              " 'these',\n",
              " 'deviations',\n",
              " 'with',\n",
              " 'little',\n",
              " 'difficulty',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CTi9G81Q0J7",
        "outputId": "b239bfbf-26d7-4b24-afe1-5f404d7fd714"
      },
      "source": [
        "print(\"train\")\n",
        "print(len(trainings_data), trainings_data[10])\n",
        "print(\"validation\")\n",
        "print(len(val_data),val_data[10])\n",
        "print(\"test\")\n",
        "print(len(test_data),test_data[10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train\n",
            "540 (['In', 'this', 'presentation', ',', 'we', 'describe', 'the', 'features', 'of', 'and', 'requirements', 'for', 'a', 'genuinely', 'useful', 'software', 'infrastructure', 'for', 'this', 'purpose', '.'], ['n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'B-T', 'n', 'n', 'n', 'n', 'B-T', 'T', 'n', 'n', 'n', 'n'])\n",
            "validation\n",
            "180 (['The', 'resulting', 'logical', 'expression', 'is', 'then', 'transformed', 'by', 'a', 'planning', 'algorithm', 'into', 'efficient', 'Prolog', ',', 'cf.', 'query', 'optimisation', 'in', 'a', 'relational', 'database', '.'], ['n', 'n', 'B-T', 'T', 'n', 'n', 'n', 'n', 'n', 'B-T', 'T', 'n', 'n', 'B-T', 'n', 'n', 'B-T', 'T', 'n', 'n', 'B-T', 'T', 'n'])\n",
            "test\n",
            "180 (['The', 'basic', 'goal', 'in', 'building', 'that', 'editor', 'was', 'to', 'provide', 'an', 'adequate', 'tool', 'to', 'help', 'lexicologists', 'produce', 'a', 'valid', 'and', 'coherent', 'dictionary', 'on', 'the', 'basis', 'of', 'a', 'linguistic', 'theory', '.'], ['n', 'n', 'n', 'n', 'n', 'n', 'B-T', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'B-T', 'n', 'n', 'n', 'n', 'n', 'B-T', 'n', 'n', 'n', 'n', 'n', 'B-T', 'T', 'n'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVxAsANXfpDv"
      },
      "source": [
        "# Tokenize "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ieMHql0gobX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115,
          "referenced_widgets": [
            "261f3489b5fa40e5a418b2f7c86a7485",
            "f238cb2cebc8432db62e2ce58501d4ee",
            "18298fcbc62342b1866ef1698a07479c",
            "2ba1651850a540148c98e28b93fd9699",
            "db1dcda6745f45f3be4db5d7ca36b7e4",
            "700cd22776464a0bade4f6ef18328d8d",
            "68a7382cd4d349fda83e520cd1bf409f",
            "97b9a7598222413e8c6715e2867ea1c8",
            "bce44ce6b1db444c847e91ef7ea2a65d",
            "bce2c67fe5bb4a61923dbfb27ce0f9be",
            "bbfc007aa13b492d939693b75245be37",
            "166e8522e25648bc824a7d8ebd05defe",
            "ae1cbd96881f4debae04380388f732be",
            "8553e6e79ea34dce95d37af0a51232ac",
            "ef5512996212415c8d23dcff350e9d20",
            "57b1fe6b1cf642618cd942635a492f2f"
          ]
        },
        "outputId": "c73595d6-e08e-4277-ec09-e9ada7ffabb7"
      },
      "source": [
        "tokenizer = XLMRobertaTokenizerFast.from_pretrained(\"xlm-roberta-base\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "261f3489b5fa40e5a418b2f7c86a7485",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=5069051.0, style=ProgressStyle(descriptâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bce44ce6b1db444c847e91ef7ea2a65d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=9096718.0, style=ProgressStyle(descriptâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYftDnmguJMr"
      },
      "source": [
        "label_list=[\"n\", \"B-T\", \"T\"]\n",
        "label_to_id = {l: i for i, l in enumerate(label_list)}\n",
        "num_labels=len(label_list)\n",
        "\n",
        "def tokenize_and_align_labels(texts, tags):\n",
        "  tokenized_inputs = tokenizer(\n",
        "      texts,\n",
        "      padding=True,\n",
        "      truncation=True,\n",
        "      # We use this argument because the texts in our dataset are lists of words (with a label for each word).\n",
        "      is_split_into_words=True,\n",
        "  )\n",
        "  labels = []\n",
        "  for i, label in enumerate(tags):\n",
        "      word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "      previous_word_idx = None\n",
        "      label_ids = []\n",
        "      for word_idx in word_ids:\n",
        "          # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
        "          # ignored in the loss function.\n",
        "          if word_idx is None:\n",
        "              label_ids.append(-100)\n",
        "          # We set the label for the first token of each word.\n",
        "          elif word_idx != previous_word_idx:\n",
        "              label_ids.append(label_to_id[label[word_idx]])\n",
        "          # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
        "          # the label_all_tokens flag.\n",
        "          else:\n",
        "              label_ids.append(-100)\n",
        "          previous_word_idx = word_idx\n",
        "\n",
        "      labels.append(label_ids)\n",
        "  tokenized_inputs[\"labels\"] = labels\n",
        "  return tokenized_inputs  \n",
        "\n",
        "\n",
        "train_input_and_labels = tokenize_and_align_labels(train_texts, train_tags)\n",
        "\n",
        "val_input_and_labels = tokenize_and_align_labels(val_texts, val_tags)\n",
        "\n",
        "test_input_and_labels = tokenize_and_align_labels(test_texts, test_tags)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lcPXbZ22yWG"
      },
      "source": [
        "# create dataset\n",
        "class OurDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = OurDataset(train_input_and_labels, train_input_and_labels[\"labels\"])\n",
        "\n",
        "val_dataset = OurDataset(val_input_and_labels, val_input_and_labels[\"labels\"])\n",
        "\n",
        "test_dataset = OurDataset(test_input_and_labels, test_input_and_labels[\"labels\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9miQ8_HxKqGB"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lE8JxaF5T9Yd"
      },
      "source": [
        "# extract set from true_predictions\n",
        "\n",
        "def extract_terms(token_predictions, val_texts):\n",
        "  extracted_terms = set()\n",
        "  # go over all predictions\n",
        "  for i in range(len(token_predictions)):\n",
        "    pred = token_predictions[i]\n",
        "    txt  = val_texts[i]\n",
        "    for j in range(len(pred)):\n",
        "      # if right tag build term and add it to the set otherwise just continue\n",
        "      if pred[j]==\"B-T\":\n",
        "        term=txt[j]\n",
        "        for k in range(j+1,len(pred)):\n",
        "          if pred[k]==\"T\": term+=\" \"+txt[k]\n",
        "          else: break\n",
        "        extracted_terms.add(term)\n",
        "  return extracted_terms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMz6v3kUNT39"
      },
      "source": [
        "# how to compute the metrics (we don't use this one for the trainer, only to get inference predictions later...)\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    # Remove ignored index (special tokens)\n",
        "    true_predictions = [\n",
        "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    return {\n",
        "        \"accuracy_score\": accuracy_score(true_labels, true_predictions),\n",
        "        \"precision\": precision_score(true_labels, true_predictions),\n",
        "        \"recall\": recall_score(true_labels, true_predictions),\n",
        "        \"f1\": f1_score(true_labels, true_predictions),\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xn5_7q3CLqt8"
      },
      "source": [
        "# how to compute the metrics TermEval style for Trainer\n",
        "# only for validation set since the gold_set is fixed to be the validation set !\n",
        "def compute_metrics_2(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    # Remove ignored index (special tokens)\n",
        "    true_predictions = [\n",
        "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    extracted_terms=extract_terms(true_predictions, val_texts) # fixed validation set!!\n",
        "    #extracted_terms = set([item.lower() for item in extracted_terms])  #for ACL we do not need to lowercase the data \n",
        "    gold_set=gold_set_for_validation      # fixed validation set!!\n",
        "    true_pos=extracted_terms.intersection(gold_set)\n",
        "    recall=len(true_pos)/len(gold_set)\n",
        "    precision=len(true_pos)/len(extracted_terms)\n",
        "\n",
        "    return {\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": 2*(precision*recall)/(precision+recall),\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220,
          "referenced_widgets": [
            "0736701407234906a42b83c2b44596d9",
            "56a4fae342704759b094c7ce651d4b2a",
            "7e633a8fd24b4c589ebe755ae113b097",
            "12dd819648df4f12b1900ba2ed6755f8",
            "a672ddfcc6ef4790817b9d1eabcdae3b",
            "7ba12e11c3f6459181dcdf13d0cea4c3",
            "011db152017047c4a29a6689c81e2142",
            "e9358badb01642dfb08c116bb336ad15",
            "dbc12261a7f44edfa047ebd868fa3641",
            "8081057dab3e45a9853ac6e28cc32f43",
            "10464b7d831f4ddf8c8ae25ea62144a0",
            "8f7a30667b404648b893fb56dba5f81c",
            "731fc42ab6aa44878e2fcfa6d9107fe4",
            "0ca1cbd92453441fba37b7717842b8cd",
            "d53564f0c90247b3a5e6cbbcea36904d",
            "b26ac46a11a344b3bf2fe2c4bb25f4f9"
          ]
        },
        "id": "yJf_Rnyf26el",
        "outputId": "3d22d4f3-3411-4bf4-8efb-ca1f0571c643"
      },
      "source": [
        "# initialize model\n",
        "model = XLMRobertaForTokenClassification.from_pretrained(\"xlm-roberta-base\", num_labels=num_labels)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0736701407234906a42b83c2b44596d9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=512.0, style=ProgressStyle(description_â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dbc12261a7f44edfa047ebd868fa3641",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1115590446.0, style=ProgressStyle(descrâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuWc6fXHK8A9"
      },
      "source": [
        "# training arguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=10,              # total # of training epochs\n",
        "    per_device_train_batch_size=8,  # batch size per device during training\n",
        "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
        "    warmup_steps=0,                  # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0,                  # strength of weight decay\n",
        "    learning_rate=2e-5,\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    evaluation_strategy= \"epoch\",#\"steps\", # or use epoch here\n",
        "    eval_steps=100,\n",
        "    #save_total_limit=1,\n",
        "    load_best_model_at_end=True,   #loads the model with the best evaluation score\n",
        "    metric_for_best_model=\"f1\",\n",
        "    greater_is_better=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEg8krxu-MY4"
      },
      "source": [
        "# initialize huggingface trainer\n",
        "trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics_2,\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZIimqAK28qS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "outputId": "76ca3e0c-158a-4bcf-d971-8eb6266260bf"
      },
      "source": [
        "# train\n",
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='680' max='680' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [680/680 12:21, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.222186</td>\n",
              "      <td>0.642127</td>\n",
              "      <td>0.745843</td>\n",
              "      <td>0.690110</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.174606</td>\n",
              "      <td>0.712719</td>\n",
              "      <td>0.771971</td>\n",
              "      <td>0.741163</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.197761</td>\n",
              "      <td>0.749380</td>\n",
              "      <td>0.717340</td>\n",
              "      <td>0.733010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.212888</td>\n",
              "      <td>0.708609</td>\n",
              "      <td>0.762470</td>\n",
              "      <td>0.734554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.204812</td>\n",
              "      <td>0.741419</td>\n",
              "      <td>0.769596</td>\n",
              "      <td>0.755245</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.239005</td>\n",
              "      <td>0.706140</td>\n",
              "      <td>0.764846</td>\n",
              "      <td>0.734322</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.256164</td>\n",
              "      <td>0.735714</td>\n",
              "      <td>0.733967</td>\n",
              "      <td>0.734839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.152900</td>\n",
              "      <td>0.270901</td>\n",
              "      <td>0.729730</td>\n",
              "      <td>0.769596</td>\n",
              "      <td>0.749133</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.152900</td>\n",
              "      <td>0.285894</td>\n",
              "      <td>0.719647</td>\n",
              "      <td>0.774347</td>\n",
              "      <td>0.745995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.152900</td>\n",
              "      <td>0.287512</td>\n",
              "      <td>0.726027</td>\n",
              "      <td>0.755344</td>\n",
              "      <td>0.740396</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=680, training_loss=0.12057683608111214, metrics={'train_runtime': 742.7479, 'train_samples_per_second': 0.916, 'total_flos': 0, 'epoch': 10.0, 'init_mem_cpu_alloc_delta': 0, 'init_mem_gpu_alloc_delta': 1110393856, 'init_mem_cpu_peaked_delta': 0, 'init_mem_gpu_peaked_delta': 0, 'train_mem_cpu_alloc_delta': -693235712, 'train_mem_gpu_alloc_delta': 3360015360, 'train_mem_cpu_peaked_delta': 702091264, 'train_mem_gpu_peaked_delta': 946824704})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovbaJVNyzw_O"
      },
      "source": [
        "trainer.save_model(\"./saved models/term_acl_1305_an1\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_dYhX6t2FsM"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcP6gMb0UQgS"
      },
      "source": [
        "#TODO REWRITE EVALUATE FUNCTION TO TAKE TXT + DF + DATASET AS PARAMETER AND THEN ITS ALL EZ PZ OMG"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kp2vASa41KBf"
      },
      "source": [
        "#load model\n",
        "PATH = \"./saved models/term_acl_1305_an1\" \n",
        "model_trained = XLMRobertaForTokenClassification.from_pretrained(PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YbAm06lBNEL"
      },
      "source": [
        "# initialize huggingface trainer with already trained model\n",
        "trainer = Trainer(\n",
        "        model=model_trained,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "1fGYhaJjx_qr",
        "outputId": "3befc17c-a59e-4822-c9de-d7a701c56e47"
      },
      "source": [
        "#en\n",
        "predictions, labels, metrics = trainer.predict(val_dataset)\n",
        "predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "# Remove ignored index (special tokens)\n",
        "true_predictions = [\n",
        "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "    for prediction, label in zip(predictions, labels)\n",
        "]\n",
        "\n",
        "#test\n",
        "test_predictions, test_labels, test_metrics = trainer.predict(test_dataset)\n",
        "test_predictions = np.argmax(test_predictions, axis=2)\n",
        "# Remove ignored index (special tokens)\n",
        "true_test_predictions = [\n",
        "    [label_list[p] for (p, l) in zip(test_prediction, test_label) if l != -100]\n",
        "    for test_prediction, test_label in zip(test_predictions, test_labels)\n",
        "]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='24' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [12/12 00:01]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: n seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: T seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuA-phCvWwdD"
      },
      "source": [
        "**List Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnY89ltMTrm6"
      },
      "source": [
        "def computeTermEvalMetrics(extracted_terms, gold_df):\n",
        "  #make lower case cause gold standard is lower case\n",
        "  #extracted_terms = set([item.lower() for item in extracted_terms])\n",
        "  gold_set=set(gold_df)\n",
        "  true_pos=extracted_terms.intersection(gold_set)\n",
        "  recall=len(true_pos)/len(gold_set)\n",
        "  precision=len(true_pos)/len(extracted_terms)\n",
        "\n",
        "  print(\"Intersection\",len(true_pos))\n",
        "  print(\"Gold\",len(gold_set))\n",
        "  print(\"Extracted\",len(extracted_terms))\n",
        "  print(\"Recall:\", recall)\n",
        "  print(\"Precision:\", precision)\n",
        "  print(\"F1:\", 2*(precision*recall)/(precision+recall))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSAjQkEAMaMq"
      },
      "source": [
        "#extracted_terms = extract_terms(true_predictions, val_texts)\n",
        "extracted_terms = extract_terms(true_predictions, val_texts)\n",
        "\n",
        "#test_extracted_terms = extract_terms(true_test_predictions, test_texts)\n",
        "test_extracted_terms = extract_terms(true_test_predictions, test_texts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHZ9r13lxd15",
        "outputId": "bb2a7ab1-abef-4edd-9f68-7c116cb5e16d"
      },
      "source": [
        "computeTermEvalMetrics(extracted_terms, gold_set_for_validation)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Intersection 317\n",
            "Gold 421\n",
            "Extracted 421\n",
            "Recall: 0.7529691211401425\n",
            "Precision: 0.7529691211401425\n",
            "F1: 0.7529691211401426\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U715AiK3JrIO",
        "outputId": "5dddbd7d-10cb-435f-ffbe-53679dc6531b"
      },
      "source": [
        "computeTermEvalMetrics(test_extracted_terms, gold_set_for_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Intersection 354\n",
            "Gold 478\n",
            "Extracted 473\n",
            "Recall: 0.7405857740585774\n",
            "Precision: 0.7484143763213531\n",
            "F1: 0.7444794952681387\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nu2EDCLa9QLU"
      },
      "source": [
        "**Analysis FP and FN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEv76efs8-D_"
      },
      "source": [
        "extracted_terms =  test_extracted_terms\n",
        "gold_set=gold_set_for_test\n",
        "true_pos=extracted_terms.intersection(gold_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPck_YJHAKuD",
        "outputId": "697b9c62-25b2-47eb-8d44-644b2664d5c7"
      },
      "source": [
        "recall=len(true_pos)/len(gold_set)\n",
        "precision=len(true_pos)/len(extracted_terms)\n",
        "\n",
        "print(\"Intersection\",len(true_pos))\n",
        "print(\"Gold\",len(gold_set))\n",
        "print(\"Extracted\",len(extracted_terms))\n",
        "print(\"Recall:\", recall)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"F1:\", 2*(precision*recall)/(precision+recall))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Intersection 354\n",
            "Gold 478\n",
            "Extracted 473\n",
            "Recall: 0.7405857740585774\n",
            "Precision: 0.7484143763213531\n",
            "F1: 0.7444794952681387\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03RCXAVcabmg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12664d13-2475-4bf2-ba95-1b141629a356"
      },
      "source": [
        "#false negatives (what was missed)\n",
        "fn=gold_set-extracted_terms\n",
        "fn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'',\n",
              " 'ATIS (Air Travel Information System) domain',\n",
              " 'C++',\n",
              " 'CSR (Connected Speech Recognition) corpus',\n",
              " 'Canadian Hansards',\n",
              " 'Chinese newspapers',\n",
              " 'DARPA Resource Management corpus',\n",
              " \"February '92 benchmark evaluation\",\n",
              " \"February '92 test sentences\",\n",
              " 'February 1992 ATIS benchmark tests',\n",
              " \"Grolier's Encyclopedia\",\n",
              " 'HMM with Gaussian mixture observation densities',\n",
              " 'IDUS',\n",
              " 'IDUS (Intelligent Document Understanding System)',\n",
              " 'LRE project SmTA double check',\n",
              " 'MADCOW (Multi-site ATIS Data COllection Working group)',\n",
              " 'MIT ATIS (Air Travel Information Service) system',\n",
              " \"NTHU's statistic-based system\",\n",
              " 'OCR',\n",
              " \"October '91 dry-run test set\",\n",
              " \"October '91 test set\",\n",
              " 'Resource Management (RM) corpus',\n",
              " \"Roget's Thesaurus\",\n",
              " 'Similarity-driven Transfer System (SimTran)',\n",
              " 'Simulated annealing approach',\n",
              " 'Spanish',\n",
              " 'Syntactic analysis of the English coordinate sentences',\n",
              " 'TDMT (Transfer-Driven Machine Translation)',\n",
              " 'TDMT on APs',\n",
              " 'adaptive dynamic word formation',\n",
              " 'adaptive learning procedure',\n",
              " 'adjoining words',\n",
              " 'and',\n",
              " 'associative processors (APs)',\n",
              " 'asynchronous message passing',\n",
              " 'automatic generation of hypertext links',\n",
              " 'bilingual material',\n",
              " 'but',\n",
              " 'case-based MT (CBMT)',\n",
              " 'closed semantic domains',\n",
              " 'collection',\n",
              " 'computationally feasible',\n",
              " 'concurrent, object-oriented natural language parsing',\n",
              " 'continuous density hidden Markov models (CDHMM)',\n",
              " 'copying',\n",
              " 'corpus of bracketed sentences',\n",
              " 'detecting speech repairs',\n",
              " 'document descriptors (keywords)',\n",
              " 'erroneous chains',\n",
              " 'example-retrieval (ER)',\n",
              " 'extended DCG formalism',\n",
              " 'generalized LR parsing',\n",
              " 'generation',\n",
              " 'generative probabilistic model of natural language',\n",
              " 'global behavior',\n",
              " 'head-oriented notions of valency and dependency',\n",
              " 'head-to-head tests',\n",
              " 'hidden Markov models (HMM)',\n",
              " 'identification of 2-character and 3-character Chinese names without title',\n",
              " 'independently trained models',\n",
              " 'instantiation scheme',\n",
              " 'insufficient training data',\n",
              " 'integration',\n",
              " 'interactive environment',\n",
              " 'interactive method for speech understanding',\n",
              " 'language understanding technology',\n",
              " 'lexical distribution',\n",
              " 'lexical, syntactic, semantic, and structural information',\n",
              " 'lexicalization',\n",
              " 'lexicon specification',\n",
              " 'likely repair',\n",
              " 'linguistically sophisticated representation of documents',\n",
              " 'linguistically sophisticated units',\n",
              " 'log(d) overheads',\n",
              " 'low precision',\n",
              " 'machine translation (MT) systems',\n",
              " 'maximum a posteriori estimation',\n",
              " 'maximum likelihood method',\n",
              " 'message passing protocols',\n",
              " 'mixture trigram models',\n",
              " 'multi-site common evaluation of speech, natural language and spoken language',\n",
              " 'n-best speech/language integration architecture',\n",
              " 'name identification',\n",
              " 'non-terminal symbol',\n",
              " 'or',\n",
              " 'parsing accuracy',\n",
              " 'parsing strategies',\n",
              " 'partial instantiation',\n",
              " 'performance',\n",
              " 'personal names',\n",
              " 'pre-tagged corpus',\n",
              " 'principle-and-parameters language framework',\n",
              " 'principle-based parser',\n",
              " \"quasi-destructive scheme's ability\",\n",
              " 'real-time response',\n",
              " 'real-time spoken language translation',\n",
              " 'recovery technique for failed parses',\n",
              " 'reduction',\n",
              " 'reduction in the search space',\n",
              " 'referential properties of noun phrases',\n",
              " 'robust analysis',\n",
              " 'robust parsing capability',\n",
              " 'sophisticated representations of documents',\n",
              " 'span',\n",
              " 'speaker adaptation (SA)',\n",
              " 'speaker-independent (SI) training',\n",
              " 'standard evaluation metric',\n",
              " 'statistical approaches',\n",
              " 'statistically fitted rule-based model',\n",
              " 'structure-sharing of graphs',\n",
              " 'syllables',\n",
              " 'target speaker',\n",
              " 'temporal relations',\n",
              " 'temporary belief',\n",
              " 'training',\n",
              " 'training (reference) speaker',\n",
              " 'transfer phase',\n",
              " 'unmodified subgraphs',\n",
              " 'unsupervised structure finding methods',\n",
              " 'user-defined performance thresholds',\n",
              " 'vocabulary',\n",
              " 'well-written discourse',\n",
              " 'word',\n",
              " 'word set based representation'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bt7TH7vGal9m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee0961db-5f93-4af9-c290-c11e237716ba"
      },
      "source": [
        "#false positives (wrongly seen as term)\n",
        "fp=extracted_terms-gold_set\n",
        "fp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'5K vocabulary',\n",
              " 'ATIS benchmark tests',\n",
              " 'C + +',\n",
              " 'DARPA',\n",
              " 'DCG formalism',\n",
              " 'English coordinate sentences',\n",
              " 'Gaussian mixture observation densities',\n",
              " 'HMM',\n",
              " 'IDUS development',\n",
              " 'LR parsing',\n",
              " 'LRE',\n",
              " 'NTHU',\n",
              " 'OCR accuracy',\n",
              " 'Resource Management corpus',\n",
              " 'SmTA double check',\n",
              " 'Syntactic analysis',\n",
              " 'TDMT on',\n",
              " 'accounts based on processing',\n",
              " 'adjoining',\n",
              " 'analysis and generation',\n",
              " 'annealing approach',\n",
              " 'applications',\n",
              " 'benchmark evaluation',\n",
              " 'bracketed sentences',\n",
              " 'browsing and editing',\n",
              " 'chains',\n",
              " 'characters',\n",
              " 'combinatorics',\n",
              " 'communications',\n",
              " 'compound noun component',\n",
              " 'computational systems',\n",
              " 'constraints',\n",
              " 'context',\n",
              " 'contextual clues',\n",
              " 'copying of unmodified subgraphs',\n",
              " 'data',\n",
              " 'data collection',\n",
              " 'discourse effect',\n",
              " 'discrimination and robustness oriented adaptive learning procedure',\n",
              " 'dry-run test',\n",
              " 'error',\n",
              " 'errors',\n",
              " 'evaluation metric',\n",
              " 'examples',\n",
              " 'frequent word',\n",
              " 'full lexicalization',\n",
              " 'further grammar',\n",
              " 'general-purpose language understanding technology',\n",
              " 'generative probabilistic model',\n",
              " 'geometric model',\n",
              " 'global scene',\n",
              " 'grammar and test',\n",
              " 'graphs',\n",
              " 'head-oriented notions',\n",
              " 'high-precision rules',\n",
              " 'hypertext links',\n",
              " 'input expression',\n",
              " 'instantiation',\n",
              " 'interactive method',\n",
              " 'interpretation',\n",
              " 'interpretations',\n",
              " 'lexical distribution of',\n",
              " 'linguistically',\n",
              " 'linking of',\n",
              " 'log',\n",
              " 'mixture',\n",
              " 'mixture language model',\n",
              " 'models',\n",
              " 'multi-site common evaluation',\n",
              " 'n-best speech',\n",
              " 'name identification capability',\n",
              " 'neighbors',\n",
              " 'noisy portion',\n",
              " 'non-redundant lexicon specification',\n",
              " 'notations',\n",
              " 'notions',\n",
              " 'noun phrases',\n",
              " 'parameters',\n",
              " 'parses',\n",
              " 'parsimonious instantiation scheme',\n",
              " 'parsing accuracy rate',\n",
              " 'parsing capability',\n",
              " 'performance thresholds',\n",
              " 'pints',\n",
              " 'precision',\n",
              " 'processing accounts',\n",
              " 'program',\n",
              " 'quasi-destructive scheme',\n",
              " 'real-time',\n",
              " 'recovery technique',\n",
              " 'referential properties',\n",
              " 'repair',\n",
              " 'response',\n",
              " 'rule-based model',\n",
              " 'scenic descriptions',\n",
              " 'semantic domains',\n",
              " 'semantic interpretation',\n",
              " 'share sense',\n",
              " 'speech recognition applications',\n",
              " 'speech recognition technology',\n",
              " 'speech understanding',\n",
              " 'speed-up element',\n",
              " 'spoken language',\n",
              " 'statistic-based system',\n",
              " 'statistical',\n",
              " 'statistically',\n",
              " 'structure-sharing of',\n",
              " 'test sentences',\n",
              " 'theoretical accounts',\n",
              " 'title',\n",
              " 'training data',\n",
              " 'translation use',\n",
              " 'understanding process',\n",
              " 'unidentified portion',\n",
              " 'units',\n",
              " 'unsupervised structure',\n",
              " 'valency and dependency',\n",
              " 'weighted error',\n",
              " 'word set'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHySDu4HEFWF",
        "outputId": "0d1dfda7-eb68-4e02-c704-1067ad6b9d14"
      },
      "source": [
        "#true pos\n",
        "true_pos"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'APs',\n",
              " 'ATNs',\n",
              " 'BU recognition system',\n",
              " 'Bayesian learning',\n",
              " 'CSR',\n",
              " 'CSR corpus',\n",
              " 'CSR pilot corpus',\n",
              " 'Chart-like parsing',\n",
              " 'Chinese Natural Language Processing',\n",
              " 'Chinese-English texts',\n",
              " 'DARPA speech recognition technology',\n",
              " 'Dynamic Grammars',\n",
              " 'ER',\n",
              " 'English',\n",
              " 'English alphabets',\n",
              " 'English coordinate structure analysis model',\n",
              " 'English sentences',\n",
              " 'English word',\n",
              " 'English words',\n",
              " 'English-Japanese MT system',\n",
              " 'Graph unification',\n",
              " 'HBG',\n",
              " 'HBG model',\n",
              " 'Horn logic program',\n",
              " 'Human-Machine Communication',\n",
              " 'IR',\n",
              " 'Japanese',\n",
              " 'Japanese bunsetsu',\n",
              " 'Japanese homophone errors',\n",
              " 'Japanese kanji-kana characters',\n",
              " 'Japanese revision support systems',\n",
              " 'Japanese texts',\n",
              " 'KANA-KANJI conversion',\n",
              " 'LHIP',\n",
              " 'LIMSI',\n",
              " 'Language understanding',\n",
              " 'MADCOW',\n",
              " 'MAP estimation approach',\n",
              " 'MIT ATIS system',\n",
              " 'MLE reestimation algorithms',\n",
              " 'MT',\n",
              " 'MT systems',\n",
              " 'MUC-3 evaluation',\n",
              " 'Markov probability',\n",
              " 'NLP techniques',\n",
              " 'Natural Language Processing',\n",
              " 'Non Verbal and Multimodal Communication',\n",
              " 'P-CFG',\n",
              " 'PC based tool',\n",
              " 'Paramax',\n",
              " 'Paramax spoken language understanding system',\n",
              " 'Parsing',\n",
              " 'SI',\n",
              " 'SI corpus',\n",
              " 'SI recognition',\n",
              " 'SPRINT',\n",
              " 'SUMMIT recognizer',\n",
              " 'SUN Sparcstations',\n",
              " 'SYSCONJ system',\n",
              " 'Spoken language translation',\n",
              " 'TACITUS',\n",
              " 'TDMT',\n",
              " 'TGE',\n",
              " 'Treebank',\n",
              " 'WI systems',\n",
              " 'Wall Street Journal text corpus',\n",
              " 'Word Identification',\n",
              " 'X-windows',\n",
              " 'abduction',\n",
              " 'abductive inference',\n",
              " 'accuracy',\n",
              " 'accuracy rate',\n",
              " 'actor paradigm',\n",
              " 'actors',\n",
              " 'adaptation',\n",
              " 'agenda-based scheduling parser',\n",
              " 'alignment algorithm',\n",
              " 'ambiguities',\n",
              " 'ambiguity',\n",
              " 'ambiguity handling',\n",
              " 'ambiguity packing',\n",
              " 'analysis',\n",
              " 'analysis cost',\n",
              " 'annotation',\n",
              " 'approximation error',\n",
              " 'arcs',\n",
              " 'averaging',\n",
              " 'balance matching operation',\n",
              " 'behavioral specification',\n",
              " 'bigram language model',\n",
              " 'bilingual texts',\n",
              " 'candidates',\n",
              " 'case-based reasoning',\n",
              " 'category transitions',\n",
              " 'chart',\n",
              " 'chart Parsing',\n",
              " 'chart-based algorithms',\n",
              " 'chart-based phrase structure parsing',\n",
              " 'clauses',\n",
              " 'coherence rules',\n",
              " 'components',\n",
              " 'compound noun',\n",
              " 'compound nouns',\n",
              " 'computation model',\n",
              " 'computer program',\n",
              " 'concurrency',\n",
              " 'concurrency of computation',\n",
              " 'concurrent computation model',\n",
              " 'conjunction',\n",
              " 'constituent',\n",
              " 'constituents',\n",
              " 'constraint',\n",
              " 'context-dependent phonetic modelling',\n",
              " 'continuous speech recognition',\n",
              " 'coordinate conjunction',\n",
              " 'coordinate conjunctions',\n",
              " 'coordination',\n",
              " 'corpora',\n",
              " 'corpus',\n",
              " 'corrective training',\n",
              " 'cyclic structures',\n",
              " 'database query paraphrase',\n",
              " 'decision tree building',\n",
              " 'dependency pointers',\n",
              " 'dependency relations',\n",
              " 'descriptors',\n",
              " 'dictionary',\n",
              " 'disambiguation algorithms',\n",
              " 'disambiguation process',\n",
              " 'discourse',\n",
              " 'discourse constraint',\n",
              " 'discourse segmentation',\n",
              " 'discourse segments',\n",
              " 'document classification',\n",
              " 'document processing',\n",
              " 'document understanding',\n",
              " 'document understanding technology',\n",
              " 'documents',\n",
              " 'domain-independent capabilities',\n",
              " 'early copying',\n",
              " 'edge',\n",
              " 'edges',\n",
              " 'editing terms',\n",
              " 'editor',\n",
              " 'ellipses',\n",
              " 'entities',\n",
              " 'entries',\n",
              " 'error characters',\n",
              " 'error rate',\n",
              " 'evaluation methodology',\n",
              " 'event networks',\n",
              " 'event type networks',\n",
              " 'exponential time',\n",
              " 'expressions',\n",
              " 'extraction and conversion rules',\n",
              " 'extrapolation',\n",
              " 'feature sets',\n",
              " 'fluent speech',\n",
              " 'forward-backward algorithm',\n",
              " 'free indexation',\n",
              " 'free indexation algorithm',\n",
              " 'function words',\n",
              " 'grammar',\n",
              " 'grammar model',\n",
              " 'grammars',\n",
              " 'grammatical knowledge',\n",
              " 'head-orientation',\n",
              " 'heuristic rules',\n",
              " 'homophone',\n",
              " 'homophone errors',\n",
              " 'image and text understanding',\n",
              " 'image understanding',\n",
              " 'implicit reference resolution',\n",
              " 'incremental grammar development',\n",
              " 'indexings',\n",
              " 'inference method',\n",
              " 'inheritance',\n",
              " 'inheritance mechanisms',\n",
              " 'interface',\n",
              " 'island-based parsing method',\n",
              " 'kanji-kana characters',\n",
              " 'knowledge-based information retrieval',\n",
              " 'language families',\n",
              " 'language model',\n",
              " 'language understanding',\n",
              " 'large vocabulary continuous speech recognition',\n",
              " 'length-based or translation-based criterion',\n",
              " 'lexical cross-relations',\n",
              " 'lexical entries',\n",
              " 'lexical generalizations',\n",
              " 'lexical rules',\n",
              " 'lexically distributed grammar',\n",
              " 'lexicologists',\n",
              " 'lexicon',\n",
              " 'lexicons',\n",
              " 'linguistic analysis',\n",
              " 'linguistic analysis component',\n",
              " 'linguistic annotation',\n",
              " 'linguistic databases',\n",
              " 'linguistic information',\n",
              " 'linguistic introspection',\n",
              " 'linguistic theory',\n",
              " 'linguistically sophisticated representations',\n",
              " 'long distance constraints',\n",
              " 'm-component mixture',\n",
              " 'm-th order Markov chain model',\n",
              " 'machine readable dictionaries',\n",
              " 'machine translation',\n",
              " 'meaning',\n",
              " 'model',\n",
              " 'monolingual material',\n",
              " 'multi-site data collection paradigm',\n",
              " 'natural grammar',\n",
              " 'natural language',\n",
              " 'natural language processing',\n",
              " 'natural language texts',\n",
              " 'non-constituent coordination',\n",
              " 'non-monotonic reasoning',\n",
              " 'noun group',\n",
              " 'numerical constraints',\n",
              " 'object-oriented grammar model',\n",
              " 'optical character recognition',\n",
              " 'over copying',\n",
              " 'paragraph',\n",
              " 'parallelism',\n",
              " 'parameter smoothing',\n",
              " 'parse',\n",
              " 'parse record',\n",
              " 'parse tree',\n",
              " 'parser',\n",
              " 'parsing',\n",
              " 'parsing complexity',\n",
              " 'parsing mechanism',\n",
              " 'part-of-speech tagger',\n",
              " 'part-of-speech-based criterion',\n",
              " 'phrase boundary heuristics',\n",
              " 'phrases',\n",
              " 'pilot system',\n",
              " 'polysemous word',\n",
              " 'polysemous words',\n",
              " 'portion',\n",
              " 'pragmatics processing',\n",
              " 'probabilistic LR parser',\n",
              " 'probabilistic parsing models',\n",
              " 'probabilistic spectral mapping',\n",
              " 'processing time',\n",
              " 'qualitative spatial constraints',\n",
              " 'question',\n",
              " 'questions',\n",
              " 're-utterance',\n",
              " 'recognition accuracy',\n",
              " 'redundant copying',\n",
              " 'reestimation formulas',\n",
              " 'reference model',\n",
              " 'representations',\n",
              " 'rules',\n",
              " 'scalability',\n",
              " 'search space',\n",
              " 'segmental k-means algorithm',\n",
              " 'segments',\n",
              " 'semantic',\n",
              " 'semantic categories',\n",
              " 'semantic frame',\n",
              " 'semantic set',\n",
              " 'semantic-head-driven generation',\n",
              " 'sense',\n",
              " 'sentence',\n",
              " 'sentences',\n",
              " 'separation margin',\n",
              " 'source texts',\n",
              " 'space',\n",
              " 'spanning edges',\n",
              " 'spatial attributes',\n",
              " 'spatial concepts',\n",
              " 'spatial descriptions',\n",
              " 'speaker',\n",
              " 'speaker adaptation',\n",
              " 'speaker group modeling',\n",
              " 'speakers',\n",
              " 'speech',\n",
              " 'speech data',\n",
              " 'speech processing',\n",
              " 'speech recognition',\n",
              " 'speech recognition word and sentence error rates',\n",
              " 'speech repairs',\n",
              " 'spoken language corpus',\n",
              " 'spoken language system',\n",
              " 'spoken language translation',\n",
              " 'spoken language understanding',\n",
              " 'spontaneous speech',\n",
              " 'statistical POS tagger',\n",
              " 'statistical models',\n",
              " 'statistical systems',\n",
              " 'statistics',\n",
              " 'structural disambiguation',\n",
              " 'structure-sharing',\n",
              " 'structures',\n",
              " 'sublanguage',\n",
              " 'symmetric patterns',\n",
              " 'syntactic ambiguity resolution',\n",
              " 'syntactic analysis',\n",
              " 'syntactic and pragmatic analysis',\n",
              " 'syntactic categories',\n",
              " 'syntactic disambiguation',\n",
              " 'syntactic structure',\n",
              " 'tagger',\n",
              " 'target texts',\n",
              " 'target word selection',\n",
              " 'technical abstracting industry',\n",
              " 'temporal anaphora resolution',\n",
              " 'term weighting statistical assignment paradigm',\n",
              " 'terminal and non-terminal edges',\n",
              " 'terminal substring parsing',\n",
              " 'test set',\n",
              " 'text',\n",
              " 'text retrieval application',\n",
              " 'text understanding',\n",
              " 'title-driven name recognition',\n",
              " 'top-down scope information',\n",
              " 'training corpus',\n",
              " 'training set',\n",
              " 'training speakers',\n",
              " 'transfer system',\n",
              " 'transition',\n",
              " 'transition probabilities',\n",
              " 'translation',\n",
              " 'translation equivalents',\n",
              " 'translation examples',\n",
              " 'trigram model',\n",
              " 'trigram models',\n",
              " 'typed feature structures',\n",
              " 'unification algorithms',\n",
              " 'unification framework',\n",
              " 'unification-based grammar parsing',\n",
              " 'unknown words',\n",
              " 'unlimited vocabulary',\n",
              " 'unrestricted texts',\n",
              " 'user',\n",
              " 'utterance',\n",
              " 'utterances',\n",
              " 'valency constraints',\n",
              " 'verb group',\n",
              " 'vocabulary size',\n",
              " 'weighted sum',\n",
              " 'word disambiguation',\n",
              " 'word error rate',\n",
              " 'word fragments',\n",
              " 'word matchings',\n",
              " 'word set representation',\n",
              " 'word-sense disambiguation algorithm',\n",
              " 'word-sense disambiguation systems',\n",
              " 'words',\n",
              " 'world',\n",
              " 'world knowledge'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    }
  ]
}